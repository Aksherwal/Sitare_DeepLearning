{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture Comparison Project\n",
    "\n",
    "## ğŸ¯ Project Overview\n",
    "Welcome to this beginner-friendly neural network comparison project! \n",
    "\n",
    "In this notebook, we'll:\n",
    "- Compare **4 different neural network architectures**\n",
    "- Test them on **5 popular datasets**\n",
    "- Create a comprehensive **results table**\n",
    "- Learn the basics of neural networks with PyTorch\n",
    "\n",
    "### ğŸ§  Neural Network Architectures:\n",
    "1. **Simple CNN** - Basic Convolutional Neural Network\n",
    "2. **Deep Neural Network (DNN)** - Fully connected layers\n",
    "3. **LeNet-Style CNN** - Classic CNN architecture\n",
    "4. **Mini ResNet** - Simple residual network\n",
    "\n",
    "### ğŸ“Š Datasets:\n",
    "1. **MNIST** - Handwritten digits (28x28, grayscale)\n",
    "2. **Fashion-MNIST** - Fashion items (28x28, grayscale)\n",
    "3. **CIFAR-10** - Objects (32x32, color)\n",
    "4. **SVHN** - Street View House Numbers (32x32, color)\n",
    "5. **STL-10** - Objects (96x96, color)\n",
    "\n",
    "Let's get started! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install torch torchvision matplotlib seaborn pandas numpy tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¨ Dataset Loading and Visualization\n",
    "\n",
    "Let's load all our datasets and see what they look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \"\"\"\n",
    "    Load and prepare all datasets with appropriate transformations.\n",
    "    Returns dictionaries containing train and test loaders for each dataset.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    \n",
    "    # MNIST Dataset (28x28 grayscale)\n",
    "    mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "    ])\n",
    "    \n",
    "    mnist_train = torchvision.datasets.MNIST(root='./data', train=True, \n",
    "                                            download=True, transform=mnist_transform)\n",
    "    mnist_test = torchvision.datasets.MNIST(root='./data', train=False, \n",
    "                                           download=True, transform=mnist_transform)\n",
    "    \n",
    "    datasets['MNIST'] = {\n",
    "        'train_loader': DataLoader(mnist_train, batch_size=128, shuffle=True),\n",
    "        'test_loader': DataLoader(mnist_test, batch_size=128, shuffle=False),\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 1,\n",
    "        'input_size': 28\n",
    "    }\n",
    "    \n",
    "    # Fashion-MNIST Dataset (28x28 grayscale)\n",
    "    fashion_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.2860,), (0.3530,))  # Fashion-MNIST mean and std\n",
    "    ])\n",
    "    \n",
    "    fashion_train = torchvision.datasets.FashionMNIST(root='./data', train=True, \n",
    "                                                     download=True, transform=fashion_transform)\n",
    "    fashion_test = torchvision.datasets.FashionMNIST(root='./data', train=False, \n",
    "                                                    download=True, transform=fashion_transform)\n",
    "    \n",
    "    datasets['Fashion-MNIST'] = {\n",
    "        'train_loader': DataLoader(fashion_train, batch_size=128, shuffle=True),\n",
    "        'test_loader': DataLoader(fashion_test, batch_size=128, shuffle=False),\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 1,\n",
    "        'input_size': 28\n",
    "    }\n",
    "    \n",
    "    # CIFAR-10 Dataset (32x32 color)\n",
    "    cifar_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # CIFAR-10 mean and std\n",
    "    ])\n",
    "    \n",
    "    cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                              download=True, transform=cifar_transform)\n",
    "    cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False, \n",
    "                                             download=True, transform=cifar_transform)\n",
    "    \n",
    "    datasets['CIFAR-10'] = {\n",
    "        'train_loader': DataLoader(cifar_train, batch_size=128, shuffle=True),\n",
    "        'test_loader': DataLoader(cifar_test, batch_size=128, shuffle=False),\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 3,\n",
    "        'input_size': 32\n",
    "    }\n",
    "    \n",
    "    # SVHN Dataset (32x32 color)\n",
    "    svhn_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4377, 0.4438, 0.4728), (0.1980, 0.2010, 0.1970))  # SVHN mean and std\n",
    "    ])\n",
    "    \n",
    "    svhn_train = torchvision.datasets.SVHN(root='./data', split='train', \n",
    "                                          download=True, transform=svhn_transform)\n",
    "    svhn_test = torchvision.datasets.SVHN(root='./data', split='test', \n",
    "                                         download=True, transform=svhn_transform)\n",
    "    \n",
    "    datasets['SVHN'] = {\n",
    "        'train_loader': DataLoader(svhn_train, batch_size=128, shuffle=True),\n",
    "        'test_loader': DataLoader(svhn_test, batch_size=128, shuffle=False),\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 3,\n",
    "        'input_size': 32\n",
    "    }\n",
    "    \n",
    "    # STL-10 Dataset (96x96 color) - We'll resize to 32x32 for consistency\n",
    "    stl_transform = transforms.Compose([\n",
    "        transforms.Resize(32),  # Resize to 32x32 for computational efficiency\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4467, 0.4398, 0.4066), (0.2603, 0.2566, 0.2713))  # STL-10 mean and std\n",
    "    ])\n",
    "    \n",
    "    stl_train = torchvision.datasets.STL10(root='./data', split='train', \n",
    "                                          download=True, transform=stl_transform)\n",
    "    stl_test = torchvision.datasets.STL10(root='./data', split='test', \n",
    "                                         download=True, transform=stl_transform)\n",
    "    \n",
    "    datasets['STL-10'] = {\n",
    "        'train_loader': DataLoader(stl_train, batch_size=128, shuffle=True),\n",
    "        'test_loader': DataLoader(stl_test, batch_size=128, shuffle=False),\n",
    "        'num_classes': 10,\n",
    "        'input_channels': 3,\n",
    "        'input_size': 32\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Load all datasets\n",
    "print(\"Loading datasets...\")\n",
    "datasets = get_datasets()\n",
    "print(\"All datasets loaded successfully!\")\n",
    "\n",
    "# Print dataset information\n",
    "for name, data in datasets.items():\n",
    "    train_size = len(data['train_loader'].dataset)\n",
    "    test_size = len(data['test_loader'].dataset)\n",
    "    print(f\"{name}: {train_size} train, {test_size} test, {data['num_classes']} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_datasets(datasets):\n",
    "    \"\"\"\n",
    "    Visualize sample images from each dataset\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    fig.suptitle('Sample Images from Each Dataset', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Class names for each dataset\n",
    "    class_names = {\n",
    "        'MNIST': [str(i) for i in range(10)],\n",
    "        'Fashion-MNIST': ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "                         'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'],\n",
    "        'CIFAR-10': ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', \n",
    "                    'Dog', 'Frog', 'Horse', 'Ship', 'Truck'],\n",
    "        'SVHN': [str(i) for i in range(10)],\n",
    "        'STL-10': ['Airplane', 'Bird', 'Car', 'Cat', 'Deer', \n",
    "                  'Dog', 'Horse', 'Monkey', 'Ship', 'Truck']\n",
    "    }\n",
    "    \n",
    "    for idx, (name, data) in enumerate(datasets.items()):\n",
    "        # Get a batch of images\n",
    "        dataiter = iter(data['train_loader'])\n",
    "        images, labels = next(dataiter)\n",
    "        \n",
    "        # Select first image\n",
    "        img = images[0]\n",
    "        label = labels[0].item()\n",
    "        \n",
    "        # Plot in first row\n",
    "        ax = axes[0, idx]\n",
    "        if img.shape[0] == 1:  # Grayscale\n",
    "            ax.imshow(img.squeeze(), cmap='gray')\n",
    "        else:  # Color\n",
    "            # Denormalize for visualization\n",
    "            img = img.clone()\n",
    "            if name == 'CIFAR-10':\n",
    "                mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1)\n",
    "            elif name == 'SVHN':\n",
    "                mean = torch.tensor([0.4377, 0.4438, 0.4728]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.1980, 0.2010, 0.1970]).view(3, 1, 1)\n",
    "            elif name == 'STL-10':\n",
    "                mean = torch.tensor([0.4467, 0.4398, 0.4066]).view(3, 1, 1)\n",
    "                std = torch.tensor([0.2603, 0.2566, 0.2713]).view(3, 1, 1)\n",
    "            \n",
    "            img = img * std + mean\n",
    "            img = torch.clamp(img, 0, 1)\n",
    "            ax.imshow(img.permute(1, 2, 0))\n",
    "        \n",
    "        ax.set_title(f'{name}\\nClass: {class_names[name][label]}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Show dataset info in second row\n",
    "        ax2 = axes[1, idx]\n",
    "        info_text = f\"Shape: {img.shape}\\nChannels: {data['input_channels']}\\nClasses: {data['num_classes']}\\nTrain: {len(data['train_loader'].dataset):,}\\nTest: {len(data['test_loader'].dataset):,}\"\n",
    "        ax2.text(0.5, 0.5, info_text, ha='center', va='center', \n",
    "                fontsize=10, transform=ax2.transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "        ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize all datasets\n",
    "visualize_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Neural Network Architectures\n",
    "\n",
    "Now let's define our 4 different neural network architectures. Each has its own strengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network\n",
    "    - Uses convolutional layers to detect patterns\n",
    "    - Good for image classification tasks\n",
    "    - Around 15 layers total\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes, input_size=32):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)  # Layer 1\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # Layer 2\n",
    "        self.relu1 = nn.ReLU()  # Layer 3\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Layer 4\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # Layer 5\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # Layer 6\n",
    "        self.relu2 = nn.ReLU()  # Layer 7\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Layer 8\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # Layer 9\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # Layer 10\n",
    "        self.relu3 = nn.ReLU()  # Layer 11\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)  # Layer 12\n",
    "        \n",
    "        # Calculate the size after convolutions\n",
    "        conv_size = input_size // 8  # After 3 pooling layers (2x2 each)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * conv_size * conv_size, 256)  # Layer 13\n",
    "        self.relu4 = nn.ReLU()  # Layer 14\n",
    "        self.dropout = nn.Dropout(0.5)  # Layer 15\n",
    "        self.fc2 = nn.Linear(256, num_classes)  # Layer 16\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = self.pool1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        \n",
    "        # Second block\n",
    "        x = self.pool2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        # Third block\n",
    "        x = self.pool3(self.relu3(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        # Flatten for fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.dropout(self.relu4(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class DeepNeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Neural Network (Fully Connected)\n",
    "    - Only uses fully connected layers\n",
    "    - Treats images as flat vectors\n",
    "    - Good baseline model\n",
    "    - Exactly 20 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes, input_size=32):\n",
    "        super(DeepNeuralNetwork, self).__init__()\n",
    "        \n",
    "        input_features = input_channels * input_size * input_size\n",
    "        \n",
    "        # Create 20 layers: 9 hidden layers + 1 output layer = 10 linear layers\n",
    "        # Plus 9 ReLU + 1 dropout = 10 activation layers = 20 total\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(input_features, 512),  # Layer 1\n",
    "            nn.ReLU(),  # Layer 2\n",
    "            nn.Linear(512, 256),  # Layer 3\n",
    "            nn.ReLU(),  # Layer 4\n",
    "            nn.Linear(256, 256),  # Layer 5\n",
    "            nn.ReLU(),  # Layer 6\n",
    "            nn.Linear(256, 128),  # Layer 7\n",
    "            nn.ReLU(),  # Layer 8\n",
    "            nn.Linear(128, 128),  # Layer 9\n",
    "            nn.ReLU(),  # Layer 10\n",
    "            nn.Linear(128, 64),  # Layer 11\n",
    "            nn.ReLU(),  # Layer 12\n",
    "            nn.Linear(64, 64),  # Layer 13\n",
    "            nn.ReLU(),  # Layer 14\n",
    "            nn.Linear(64, 32),  # Layer 15\n",
    "            nn.ReLU(),  # Layer 16\n",
    "            nn.Linear(32, 32),  # Layer 17\n",
    "            nn.ReLU(),  # Layer 18\n",
    "            nn.Dropout(0.5),  # Layer 19\n",
    "            nn.Linear(32, num_classes)  # Layer 20\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Pass through all layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "class LeNetStyleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    LeNet-Style CNN (Classic Architecture)\n",
    "    - Based on the famous LeNet architecture\n",
    "    - Simple and effective for digit recognition\n",
    "    - Around 12 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes, input_size=32):\n",
    "        super(LeNetStyleCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 20, kernel_size=5)  # Layer 1\n",
    "        self.relu1 = nn.ReLU()  # Layer 2\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)  # Layer 3\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)  # Layer 4\n",
    "        self.relu2 = nn.ReLU()  # Layer 5\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)  # Layer 6\n",
    "        \n",
    "        # Calculate size after convolutions\n",
    "        # After conv1 (kernel=5): size - 4\n",
    "        # After pool1: (size - 4) // 2\n",
    "        # After conv2 (kernel=5): ((size - 4) // 2) - 4\n",
    "        # After pool2: (((size - 4) // 2) - 4) // 2\n",
    "        conv_size = (((input_size - 4) // 2) - 4) // 2\n",
    "        \n",
    "        # Classifier layers\n",
    "        self.fc1 = nn.Linear(50 * conv_size * conv_size, 500)  # Layer 7\n",
    "        self.relu3 = nn.ReLU()  # Layer 8\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Layer 9\n",
    "        \n",
    "        self.fc2 = nn.Linear(500, 84)  # Layer 10\n",
    "        self.relu4 = nn.ReLU()  # Layer 11\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Layer 12\n",
    "        \n",
    "        self.fc3 = nn.Linear(84, num_classes)  # Layer 13\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.dropout1(self.relu3(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu4(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic residual block for Mini ResNet\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class MiniResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Mini ResNet (Residual Network)\n",
    "    - Uses residual connections to help with deep networks\n",
    "    - Can train deeper networks more effectively\n",
    "    - Around 18 layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, num_classes, input_size=32):\n",
    "        super(MiniResNet, self).__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(input_channels, 16, kernel_size=3, \n",
    "                              stride=1, padding=1, bias=False)  # Layer 1\n",
    "        self.bn1 = nn.BatchNorm2d(16)  # Layer 2\n",
    "        self.relu = nn.ReLU(inplace=True)  # Layer 3\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(16, 16, 2, stride=1)  # Layers 4-9\n",
    "        self.layer2 = self._make_layer(16, 32, 2, stride=2)  # Layers 10-15\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Layer 16\n",
    "        self.dropout = nn.Dropout(0.5)  # Layer 17\n",
    "        self.fc = nn.Linear(32, num_classes)  # Layer 18\n",
    "        \n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial layers\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Residual blocks\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Create model instances to show architecture info\n",
    "models_info = {\n",
    "    'SimpleCNN': SimpleCNN,\n",
    "    'DeepNeuralNetwork': DeepNeuralNetwork, \n",
    "    'LeNetStyleCNN': LeNetStyleCNN,\n",
    "    'MiniResNet': MiniResNet\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Model Architecture Summary:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, model_class in models_info.items():\n",
    "    # Create model instance for CIFAR-10 (most common case)\n",
    "    model = model_class(input_channels=3, num_classes=10, input_size=32)\n",
    "    params = count_parameters(model)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  - Parameters: {params:,}\")\n",
    "    print(f\"  - Description: {model.__doc__.strip().split('.')[0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Training and Evaluation Functions\n",
    "\n",
    "Now let's create functions to train and evaluate our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=10, lr=0.001):\n",
    "    \"\"\"\n",
    "    Train a model and return training history\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network model to train\n",
    "        train_loader: Training data loader\n",
    "        test_loader: Test data loader\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing training history and final accuracies\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    \n",
    "    print(f\"Training for {epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # Progress bar for training\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            train_total += target.size(0)\n",
    "            train_correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_acc = 100. * train_correct / train_total\n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Acc': f'{train_acc:.2f}%'})\n",
    "        \n",
    "        # Calculate epoch training metrics\n",
    "        epoch_train_loss = train_loss / len(train_loader)\n",
    "        epoch_train_acc = 100. * train_correct / train_total\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pbar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{epochs} [Test]', leave=False)\n",
    "            \n",
    "            for data, target in test_pbar:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                test_total += target.size(0)\n",
    "                test_correct += predicted.eq(target).sum().item()\n",
    "                \n",
    "                # Update progress bar\n",
    "                test_acc = 100. * test_correct / test_total\n",
    "                test_pbar.set_postfix({'Loss': f'{loss.item():.4f}', 'Acc': f'{test_acc:.2f}%'})\n",
    "        \n",
    "        # Calculate epoch test metrics\n",
    "        epoch_test_loss = test_loss / len(test_loader)\n",
    "        epoch_test_acc = 100. * test_correct / test_total\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['test_loss'].append(epoch_test_loss)\n",
    "        history['test_acc'].append(epoch_test_acc)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}: Train Acc: {epoch_train_acc:.2f}%, Test Acc: {epoch_test_acc:.2f}%')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Final evaluation\n",
    "    final_train_acc = history['train_acc'][-1]\n",
    "    final_test_acc = history['test_acc'][-1]\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed in {training_time:.2f} seconds\")\n",
    "    print(f\"Final Training Accuracy: {final_train_acc:.2f}%\")\n",
    "    print(f\"Final Test Accuracy: {final_test_acc:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'history': history,\n",
    "        'final_train_acc': final_train_acc,\n",
    "        'final_test_acc': final_test_acc,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_training_history(history, title):\n",
    "    \"\"\"\n",
    "    Plot training history (loss and accuracy)\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], label='Training Loss', marker='o')\n",
    "    ax1.plot(history['test_loss'], label='Test Loss', marker='s')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], label='Training Accuracy', marker='o')\n",
    "    ax2.plot(history['test_acc'], label='Test Accuracy', marker='s')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ¯ Training functions ready!\")\n",
    "print(\"We'll train each model for 10 epochs with Adam optimizer.\")\n",
    "print(\"This might take a while, so grab a coffee! â˜•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Run All Experiments\n",
    "\n",
    "Now comes the exciting part! Let's train all models on all datasets and collect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "EPOCHS = 10  # You can increase this for better results\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Results storage\n",
    "results = []\n",
    "\n",
    "print(\"ğŸš€ Starting comprehensive neural network comparison!\")\n",
    "print(f\"Training {len(models_info)} models on {len(datasets)} datasets\")\n",
    "print(f\"Total experiments: {len(models_info) * len(datasets)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Counter for progress\n",
    "experiment_count = 0\n",
    "total_experiments = len(models_info) * len(datasets)\n",
    "\n",
    "# Run all experiments\n",
    "for model_name, model_class in models_info.items():\n",
    "    print(f\"\\nğŸ¤– Testing {model_name}\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for dataset_name, dataset_info in datasets.items():\n",
    "        experiment_count += 1\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Experiment {experiment_count}/{total_experiments}\")\n",
    "        print(f\"Model: {model_name} | Dataset: {dataset_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Create model instance\n",
    "            model = model_class(\n",
    "                input_channels=dataset_info['input_channels'],\n",
    "                num_classes=dataset_info['num_classes'],\n",
    "                input_size=dataset_info['input_size']\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            result = train_model(\n",
    "                model=model,\n",
    "                train_loader=dataset_info['train_loader'],\n",
    "                test_loader=dataset_info['test_loader'],\n",
    "                epochs=EPOCHS,\n",
    "                lr=LEARNING_RATE\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Dataset': dataset_name,\n",
    "                'Train_Accuracy': result['final_train_acc'],\n",
    "                'Test_Accuracy': result['final_test_acc'],\n",
    "                'Training_Time': result['training_time'],\n",
    "                'Parameters': count_parameters(model)\n",
    "            })\n",
    "            \n",
    "            # Plot training history\n",
    "            plot_training_history(\n",
    "                result['history'], \n",
    "                f\"{model_name} on {dataset_name}\"\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… Completed: {model_name} on {dataset_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error with {model_name} on {dataset_name}: {str(e)}\")\n",
    "            # Store failed result\n",
    "            results.append({\n",
    "                'Model': model_name,\n",
    "                'Dataset': dataset_name,\n",
    "                'Train_Accuracy': 0.0,\n",
    "                'Test_Accuracy': 0.0,\n",
    "                'Training_Time': 0.0,\n",
    "                'Parameters': 0\n",
    "            })\n",
    "\n",
    "print(\"\\nğŸ‰ All experiments completed!\")\n",
    "print(f\"Total time for all experiments: {sum(r['Training_Time'] for r in results):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Results Analysis and Comparison Table\n",
    "\n",
    "Let's analyze our results and create comprehensive comparison tables!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"ğŸ“Š COMPREHENSIVE RESULTS TABLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Display full results table\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Format the results for better display\n",
    "display_df = results_df.copy()\n",
    "display_df['Train_Accuracy'] = display_df['Train_Accuracy'].apply(lambda x: f\"{x:.2f}%\")\n",
    "display_df['Test_Accuracy'] = display_df['Test_Accuracy'].apply(lambda x: f\"{x:.2f}%\")\n",
    "display_df['Training_Time'] = display_df['Training_Time'].apply(lambda x: f\"{x:.1f}s\")\n",
    "display_df['Parameters'] = display_df['Parameters'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "print(display_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Create pivot tables for better visualization\n",
    "print(\"\\nğŸ“ˆ TEST ACCURACY COMPARISON TABLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pivot table for test accuracy\n",
    "accuracy_pivot = results_df.pivot(index='Model', columns='Dataset', values='Test_Accuracy')\n",
    "print(accuracy_pivot.round(2))\n",
    "\n",
    "print(\"\\nâ±ï¸ TRAINING TIME COMPARISON (seconds)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pivot table for training time\n",
    "time_pivot = results_df.pivot(index='Model', columns='Dataset', values='Training_Time')\n",
    "print(time_pivot.round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "plt.style.use('default')\n",
    "\n",
    "# 1. Test Accuracy Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(accuracy_pivot, annot=True, cmap='RdYlGn', fmt='.1f', \n",
    "            cbar_kws={'label': 'Test Accuracy (%)'}, square=True)\n",
    "plt.title('Test Accuracy Comparison Across Models and Datasets', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Model Performance Bar Chart\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "datasets_list = list(datasets.keys())\n",
    "\n",
    "for idx, dataset in enumerate(datasets_list):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    \n",
    "    # Filter data for current dataset\n",
    "    dataset_data = results_df[results_df['Dataset'] == dataset]\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = axes[row, col].bar(dataset_data['Model'], dataset_data['Test_Accuracy'], \n",
    "                             color=plt.cm.Set3(np.arange(len(dataset_data))))\n",
    "    \n",
    "    axes[row, col].set_title(f'{dataset}', fontsize=14, fontweight='bold')\n",
    "    axes[row, col].set_ylabel('Test Accuracy (%)', fontsize=10)\n",
    "    axes[row, col].tick_params(axis='x', rotation=45)\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, dataset_data['Test_Accuracy']):\n",
    "        axes[row, col].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                           f'{value:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Remove the 6th subplot (since we only have 5 datasets)\n",
    "fig.delaxes(axes[1, 2])\n",
    "\n",
    "plt.suptitle('Model Performance Comparison by Dataset', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Overall Model Ranking\n",
    "model_avg_accuracy = results_df.groupby('Model')['Test_Accuracy'].mean().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_avg_accuracy.index, model_avg_accuracy.values, \n",
    "               color=['gold', 'silver', '#CD7F32', 'lightcoral'])  # Gold, Silver, Bronze, etc.\n",
    "plt.title('Average Test Accuracy Across All Datasets', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Average Test Accuracy (%)', fontsize=12)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, value in zip(bars, model_avg_accuracy.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{value:.1f}%', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add ranking numbers\n",
    "for i, (bar, model) in enumerate(zip(bars, model_avg_accuracy.index)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2,\n",
    "             f'#{i+1}', ha='center', va='center', fontsize=14, fontweight='bold', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Analysis\n",
    "print(\"ğŸ“ˆ STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Best performing model for each dataset\n",
    "print(\"ğŸ† BEST MODEL FOR EACH DATASET:\")\n",
    "for dataset in datasets.keys():\n",
    "    dataset_results = results_df[results_df['Dataset'] == dataset]\n",
    "    best_model = dataset_results.loc[dataset_results['Test_Accuracy'].idxmax()]\n",
    "    print(f\"{dataset:12} -> {best_model['Model']:15} ({best_model['Test_Accuracy']:.2f}%)\")\n",
    "\n",
    "print(\"\\nğŸ¥‡ OVERALL MODEL RANKINGS:\")\n",
    "for i, (model, avg_acc) in enumerate(model_avg_accuracy.items(), 1):\n",
    "    medal = \"ğŸ¥‡\" if i == 1 else \"ğŸ¥ˆ\" if i == 2 else \"ğŸ¥‰\" if i == 3 else \"  \"\n",
    "    print(f\"{medal} #{i}: {model:15} - {avg_acc:.2f}% average\")\n",
    "\n",
    "# Model complexity vs performance\n",
    "print(\"\\nâš–ï¸ MODEL COMPLEXITY vs PERFORMANCE:\")\n",
    "complexity_analysis = results_df.groupby('Model').agg({\n",
    "    'Test_Accuracy': 'mean',\n",
    "    'Parameters': 'first',\n",
    "    'Training_Time': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "complexity_analysis['Efficiency'] = complexity_analysis['Test_Accuracy'] / (complexity_analysis['Parameters'] / 1000)\n",
    "complexity_analysis = complexity_analysis.sort_values('Efficiency', ascending=False)\n",
    "\n",
    "print(complexity_analysis)\n",
    "\n",
    "# Dataset difficulty ranking\n",
    "print(\"\\nğŸ“Š DATASET DIFFICULTY RANKING (by average accuracy):\")\n",
    "dataset_difficulty = results_df.groupby('Dataset')['Test_Accuracy'].mean().sort_values(ascending=False)\n",
    "for i, (dataset, avg_acc) in enumerate(dataset_difficulty.items(), 1):\n",
    "    difficulty = \"Easy\" if avg_acc > 80 else \"Medium\" if avg_acc > 60 else \"Hard\"\n",
    "    print(f\"#{i}: {dataset:12} - {avg_acc:.2f}% ({difficulty})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Insights and Learning Summary\n",
    "\n",
    "Let's summarize what we learned from this comprehensive comparison!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“ KEY INSIGHTS AND LEARNING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nğŸ§  NEURAL NETWORK ARCHITECTURE INSIGHTS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "insights = [\n",
    "    \"ğŸ” CNNs (Convolutional Neural Networks):\",\n",
    "    \"   â€¢ Best for image data due to spatial feature detection\",\n",
    "    \"   â€¢ Use convolution + pooling to reduce dimensionality\",\n",
    "    \"   â€¢ Great at detecting patterns, edges, and textures\",\n",
    "    \"\",\n",
    "    \"ğŸ§® DNNs (Deep Neural Networks):\",\n",
    "    \"   â€¢ Simple fully-connected layers\",\n",
    "    \"   â€¢ Treat images as flat vectors (lose spatial info)\",\n",
    "    \"   â€¢ Good baseline but usually outperformed by CNNs\",\n",
    "    \"\",\n",
    "    \"ğŸ›ï¸ LeNet-Style CNNs:\",\n",
    "    \"